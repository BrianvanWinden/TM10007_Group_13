{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Code opgeschoond en werkend.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "cF2F_jjxW_wK",
        "_AECMogeXS0K"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cF2F_jjxW_wK"
      },
      "source": [
        "## Assignment arrhythmia classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1-5TqoFW8_e",
        "outputId": "a443dce6-9d40-4e05-a97f-ea4be257657d"
      },
      "source": [
        "# Run this to use from colab environment\n",
        "!git clone https://github.com/BrianvanWinden/TM10007_Group_13.git\n",
        "!unzip \"/content/TM10007_Group_13/ecg/ecg_data.zip\"\n",
        "import pandas as pd \n",
        "\n",
        "data = pd.read_csv(\"ecg_data.csv\") \n",
        "\n",
        "# print(data.head())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'TM10007_Group_13'...\n",
            "remote: Enumerating objects: 87, done.\u001b[K\n",
            "remote: Counting objects: 100% (87/87), done.\u001b[K\n",
            "remote: Compressing objects: 100% (71/71), done.\u001b[K\n",
            "remote: Total 87 (delta 38), reused 47 (delta 14), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (87/87), done.\n",
            "Archive:  /content/TM10007_Group_13/ecg/ecg_data.zip\n",
            "  inflating: ecg_data.csv            \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFrCH_KzXOBE"
      },
      "source": [
        "# Importing functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5x3ZkH7XPa0"
      },
      "source": [
        "# General packages\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn import datasets as ds\n",
        "from scipy.stats import randint\n",
        "from zipfile import ZipFile\n",
        "from scipy import stats\n",
        "\n",
        "\n",
        "# Classifiers and kernels\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA, QuadraticDiscriminantAnalysis\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn import preprocessing, metrics\n",
        "from sklearn.kernel_approximation import RBFSampler\n",
        "from sklearn.metrics.pairwise import rbf_kernel, sigmoid_kernel\n",
        "from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, accuracy_score, f1_score, roc_auc_score\n",
        "\n",
        "# Regularization\n",
        "from sklearn.linear_model import Lasso, RidgeClassifier\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "# Model selection\n",
        "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, train_test_split, learning_curve, ShuffleSplit, StratifiedKFold\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Sampling\n",
        "from imblearn.combine import SMOTEENN\n",
        "from imblearn.under_sampling import EditedNearestNeighbours\n",
        "\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "\n",
        "\n",
        "def evaluation(y_val, y_pred, title = 'Confusion Matrix'):\n",
        "    cm = confusion_matrix(y_val, y_pred)\n",
        "    precision = precision_score(y_val, y_pred)\n",
        "    recall = recall_score(y_val, y_pred)\n",
        "    accuracy = accuracy_score(y_val, y_pred)\n",
        "    f1 = f1_score(y_val, y_pred)\n",
        "    # print('Recall: ', recall)\n",
        "    # print('Accuracy: ', accuracy)\n",
        "    # print('Precision: ', precision)\n",
        "    # print('F1: ', f1)\n",
        "    sns.heatmap(cm,  cmap= 'PuBu', annot=True, fmt='g', annot_kws=    {'size':20})\n",
        "    plt.xlabel('predicted', fontsize=18)\n",
        "    plt.ylabel('actual', fontsize=18)\n",
        "    plt.title(title, fontsize=18)\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AECMogeXS0K"
      },
      "source": [
        "# Taking a look at the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQkqAm3RXWhd",
        "outputId": "464f1dd6-61e1-45e8-f9bf-950e6bd6cff5"
      },
      "source": [
        "# Samples and features\n",
        "print(f'The number of samples: {len(data.index)}')\n",
        "print(f'The number of features: {len(data.columns)}')\n",
        "healthy_patients = (list(data['label'] == 0)).count(True)\n",
        "print(f'The number of healthy patients: {healthy_patients}') \n",
        "sick_patients = (list(data['label'] == 1)).count(True)\n",
        "print(f'The number of sick patients: {sick_patients}')\n",
        "percentage_sick=sick_patients/(sick_patients+healthy_patients)\n",
        "print(f'Percentage of sick patients: {round(percentage_sick,2)*100}%')\n",
        "\n",
        "# Outliers\n",
        "z = np.abs(stats.zscore(data))\n",
        "x = np.where(z>3)\n",
        "print(f'Number of features where the Z score is larger {len(x[1])}')\n",
        "\n",
        "# Missing values\n",
        "missing_values = data.isna().sum()\n",
        "number_missing_values = missing_values.astype(bool).sum(axis=0)\n",
        "print(f'Number of missig data points:{number_missing_values}')\n",
        "\n",
        "# Data\n",
        "data_points= data.drop(['label'], axis=1).to_numpy()\n",
        "data_labels= data['label'].to_numpy()\n",
        "\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of samples: 827\n",
            "The number of features: 9002\n",
            "The number of healthy patients: 681\n",
            "The number of sick patients: 146\n",
            "Percentage of sick patients: 18.0%\n",
            "Number of features where the Z score is larger 57812\n",
            "Number of missig data points:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0ddx2i7Xajb"
      },
      "source": [
        "# Preprocessing and classifier validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zaqV1Y1YXbar",
        "outputId": "43707251-f0ae-4401-d657-4a293cad8bd9"
      },
      "source": [
        "# Splitting data  \n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
        "\n",
        "x_train_val, x_test, y_train_val, y_test = train_test_split(data_points, data_labels, test_size=0.2, stratify=data_labels)\n",
        "\n",
        "cv_outer = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
        "cv_inner = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
        "outer_results = list()\n",
        "all_scores = list()\n",
        "models = list()\n",
        "\n",
        "for train_ix, val_ix in cv_outer.split(x_train_val,y_train_val):\n",
        "  # split data\n",
        "  X_train, X_val = x_train_val[train_ix, :], x_train_val[val_ix, :]\n",
        "  y_train, y_val = y_train_val[train_ix], y_train_val[val_ix]\n",
        "\n",
        "  #Resampling\n",
        "  # combines SMOTE over and edited nearest neighbour undersampling\n",
        "  # edited nearest neighbours kan van beide groepen weghalen, met sampling_strategy='majority' alleen van grote groep\n",
        "  # the authors comment that ENN is more aggressive at downsampling the majority class than Tomek Links, providing more in-depth cleaning. They apply the method, removing examples from both the majority and minority classes.\n",
        "  #A Study of the Behavior of Several Methods for Balancing Machine Learning Training Data, 2004.\n",
        "  resample = SMOTEENN(enn=EditedNearestNeighbours(sampling_strategy='majority'))\n",
        "  X_resampled, y_resampled = resample.fit_resample(X_train, y_train)\n",
        "\n",
        "  #Scaling\n",
        "  scaler = preprocessing.RobustScaler()\n",
        "  scaler.fit(X_resampled)\n",
        "  x_train_scaled = scaler.transform(X_resampled)\n",
        "  x_val_scaled = scaler.transform(X_val)\n",
        "\n",
        "  #PCA\n",
        "  pca = PCA(n_components=0.99)\n",
        "  pca = pca.fit(x_train_scaled)\n",
        "  x_train_trans = pca.transform(x_train_scaled)\n",
        "  x_val_trans = pca.transform(x_val_scaled)\n",
        "\n",
        "  model_params = {'svm': \n",
        "                  {'model': SVC(class_weight='balanced'),\n",
        "                    'params': {'C': [0.01, 0.5, 1, 10],\n",
        "                               'gamma': [0.01, 1, 10],\n",
        "                                'kernel': ['rbf', 'linear']\n",
        "                               }\n",
        "                   },\n",
        "                  'linsvc': \n",
        "                  {'model': LinearSVC(class_weight='balanced', dual=False, max_iter=10000),\n",
        "                   'params':{'C': [0.01, 0.5, 1, 10]\n",
        "                             }\n",
        "                  },\n",
        "                  'logreg':\n",
        "                  {'model': LogisticRegression(class_weight='balanced', dual=False, max_iter=10000),\n",
        "                   'params':{'C': [0.01, 0.5, 1, 10]\n",
        "                             }                      \n",
        "                  }\n",
        "                }\n",
        "  for model_name, mp in model_params.items():\n",
        "    search = GridSearchCV(mp['model'], mp['params'], cv = cv_inner, scoring = 'roc_auc', return_train_score=True)\n",
        "    result = search.fit(x_train_trans, y_resampled)\n",
        "    best_model = result.best_estimator_\n",
        "    models.append(best_model)\n",
        "    yhat = best_model.predict(x_val_trans)\n",
        "    auc_score_val = roc_auc_score(y_val, yhat)\n",
        "    outer_results.append(auc_score_val)\n",
        "    all_scores.append(result.best_score_)\n",
        "    print(f'auc: {auc_score_val}, result.best_score_:{result.best_score_}, best parameters:{result.best_params_}')\n",
        "\n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "auc: 0.5, result.best_score_:0.9639737741371135, best parameters:{'C': 1, 'gamma': 0.01, 'kernel': 'rbf'}\n",
            "auc: 0.6995412844036697, result.best_score_:0.9802374061902193, best parameters:{'C': 1}\n",
            "auc: 0.6882645259938837, result.best_score_:0.9584228511633232, best parameters:{'C': 0.01}\n",
            "auc: 0.5, result.best_score_:0.9603245917220509, best parameters:{'C': 0.5, 'gamma': 0.01, 'kernel': 'rbf'}\n",
            "auc: 0.5949074074074074, result.best_score_:0.9553097109585315, best parameters:{'C': 0.5}\n",
            "auc: 0.6041666666666667, result.best_score_:0.9510185796256575, best parameters:{'C': 0.01}\n",
            "auc: 0.5, result.best_score_:0.9658037347692521, best parameters:{'C': 0.5, 'gamma': 0.01, 'kernel': 'rbf'}\n",
            "auc: 0.5845632229756681, result.best_score_:0.9639018328673501, best parameters:{'C': 10}\n",
            "auc: 0.6314319904268049, result.best_score_:0.9663525594560077, best parameters:{'C': 0.01}\n",
            "auc: 0.5, result.best_score_:0.9626847290640395, best parameters:{'C': 0.5, 'gamma': 0.01, 'kernel': 'rbf'}\n",
            "auc: 0.6932588751495811, result.best_score_:0.967817916438606, best parameters:{'C': 1}\n",
            "auc: 0.7229756681292381, result.best_score_:0.9694106914796571, best parameters:{'C': 0.01}\n",
            "auc: 0.5, result.best_score_:0.9630148050248974, best parameters:{'C': 0.5, 'gamma': 0.01, 'kernel': 'rbf'}\n",
            "auc: 0.5364978061428001, result.best_score_:0.9782984233783225, best parameters:{'C': 1}\n",
            "auc: 0.6084962106102911, result.best_score_:0.9670244436434512, best parameters:{'C': 0.01}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C10micdFcCng"
      },
      "source": [
        "# Best classifier for test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqnxvM-GcE4T",
        "outputId": "38cf25c2-b0fb-463c-a559-42cfcc14d1ae"
      },
      "source": [
        "from imblearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "models_df = pd.DataFrame(models)\n",
        "outer_results_df = pd.DataFrame(outer_results)\n",
        "all_scores_df = pd.DataFrame(all_scores)\n",
        "results = pd.concat([models_df, outer_results_df, all_scores_df], axis=1)\n",
        "results.columns = ['Models', 'Auc score', 'Best score']\n",
        "results_sorted = results.sort_values(by=['Auc score'], ascending=False)\n",
        "# print(results_sorted)\n",
        "\n",
        "best_classifier = results_sorted['Models'].iloc[0]\n",
        "print(best_classifier)\n",
        "\n",
        "cv_test = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
        "clf = Pipeline([('resampling', SMOTEENN(enn=EditedNearestNeighbours(sampling_strategy='majority'))),('scaling', preprocessing.RobustScaler()),('pca', PCA(n_components=0.99)), ('clf', best_classifier)])\n",
        "cross_score = cross_val_score(clf, x_test, y_test, cv=cv_test, scoring='roc_auc')\n",
        "print(mean(cross_score))\n",
        "\n",
        "# #Resampling\n",
        "# resample = SMOTEENN(enn=EditedNearestNeighbours(sampling_strategy='majority'))\n",
        "# X_resampled, y_resampled = resample.fit_resample(x_train_val, y_train_val)\n",
        "\n",
        "# #Scaling\n",
        "# scaler = preprocessing.RobustScaler()\n",
        "# scaler.fit(X_resampled)\n",
        "# x_train_scaled = scaler.transform(X_resampled)\n",
        "# x_test_scaled = scaler.transform(x_test)\n",
        "\n",
        "# #PCA\n",
        "# pca = PCA(n_components=0.99)\n",
        "# pca = pca.fit(x_train_scaled)\n",
        "# x_train_trans = pca.transform(x_train_scaled)\n",
        "# x_test_trans = pca.transform(x_test_scaled)\n",
        "\n",
        "# # Evaluation classifier\n",
        "# clf = best_classifier.fit(x_train_trans, y_resampled)\n",
        "# y_pred = clf.predict(x_test_trans)\n",
        "# accuracy = metrics.balanced_accuracy_score(y_test, y_pred)\n",
        "# auc = metrics.roc_auc_score(y_test, y_pred)\n",
        "# print('Misclassified: %d / %d' % ((y_test != y_pred).sum(), x_test.shape[0]), str(clf))\n",
        "# evaluation(y_test, y_pred)\n",
        "\n",
        "# print(f'Auc score test data: {auc}')\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LogisticRegression(C=0.01, class_weight='balanced', dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=10000, multi_class='auto', n_jobs=None,\n",
            "                   penalty='l2', random_state=None, solver='lbfgs', tol=0.0001,\n",
            "                   verbose=0, warm_start=False)\n",
            "0.7278835978835978\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}