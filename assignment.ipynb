{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7SXpaKwwGe5x"
   },
   "source": [
    "# TM10007 Assignment template"
   ]
  },
  {
   "cell_type": "code",
   "exec''ution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "CiDn2Sk-VWqE",
    "outputId": "64224cd2-6054-4b04-a3f6-af8290400dfc"
   },
   "outputs": [],
   "source": [
    "# Run this to use from colab environment\n",
    "#!git clone https://github.com/BrianvanWinden/TM10007_Group_13.git\n",
    "    #!unzip \"/content/TM10007_Group_13/ecg/ecg_data.zip\"\n",
    "    # import pandas as pd \n",
    "\n",
    "    # data = pd.read_csv(\"ecg_data.csv\") \n",
    "\n",
    "    # print(data.head())"
   ]
  },
  {
   "source": [
    "## Data loading and cleaning\n",
    "\n",
    "Below are functions to load the dataset of your choice. After that, it is all up to you to create and evaluate a classification method. Beware, there may be missing values in these datasets. Good luck!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General packages\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn import datasets as ds\n",
    "from scipy.stats import randint\n",
    "from zipfile import ZipFile\n",
    "from scipy import stats\n",
    "\n",
    "# Classifiers and kernels\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA, QuadraticDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Regularization\n",
    "from sklearn.linear_model import Lasso, RidgeClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# Model selection\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, train_test_split, learning_curve, ShuffleSplit\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n"
   ]
  },
  {
   "source": [
    "## Definition of functions\n",
    "- Preprocessing data\n",
    "- Learning curves\n",
    "- Colorplots\n",
    "- Evaluation (confusion matrix)\n",
    "- Cross-validation pipeline"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data loading functions\n",
    "\n",
    "def preprocessing_data(data):\n",
    "    data_points= data.drop(['label'], axis=1).to_numpy()\n",
    "    data_labels= data['label'].to_numpy()\n",
    "\n",
    "    sick = sum(data_labels) # 146 \n",
    "\n",
    "    # Missing values\n",
    "    missing_values = data.isna().sum()\n",
    "    number_missing_values = missing_values.astype(bool).sum(axis=0)\n",
    "\n",
    "    # checking for outliers\n",
    "    z = np.abs(stats.zscore(data))\n",
    "    x = np.where(z>3)\n",
    "    #print(len(x[1])) # number of outliers -> robust scaler\n",
    "\n",
    "    # Splitting\n",
    "    # lecture 2.3 20% test set, and 15% van de train set is validatie\n",
    "    x_train_val, x_test, y_train_val, y_test = train_test_split(data_points, data_labels, test_size=0.2)\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_train_val, y_train_val, test_size=0.15)\n",
    "\n",
    "    #Scaling\n",
    "    # Robust scaler omdat er mogelijk sprake is van outliers, median wordt er dan vanaf gehaald en niet de mean die wordt beinvloed door de outliers.\n",
    "    scaler = preprocessing.RobustScaler()\n",
    "    scaler.fit(x_train)\n",
    "    x_train_scaled = scaler.transform(x_train)\n",
    "    x_val_scaled = scaler.transform(x_val)\n",
    "    x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "    #PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    pca = pca.fit(x_train_scaled)\n",
    "    x_train_trans = pca.transform(x_train_scaled)\n",
    "    x_val_trans = pca.transform(x_test_scaled)\n",
    "    x_test_trans = pca.transform(x_test_scaled)\n",
    "    explained_variance = pca.explained_variance_ratio_\n",
    "    # two pc's had a combined explained variance of 0.85 so chosing 2 pc's\n",
    "\n",
    "    # x_train_trans_df = pd.DataFrame(x_train_trans, columns=['PC1', 'PC2', 'PC3', 'PC4'])\n",
    "    # y_train_df = pd.DataFrame(y_train, columns=['label'])\n",
    "    # xy_plot = pd.concat([x_train_trans_df, y_train_df], axis=1)\n",
    "    # pc_pairplot = sns.pairplot(xy_plot, hue='label', palette='tab10')\n",
    "    \n",
    "    return x_train_trans, y_train, x_test_trans, y_test, x_val_trans, y_val\n",
    "\n",
    "\n",
    "# Learning curve\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, axes, ylim=None, cv=None,\n",
    "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \n",
    "\n",
    "    axes.set_title(title)\n",
    "    if ylim is not None:\n",
    "        axes.set_ylim(*ylim)\n",
    "    axes.set_xlabel(\"Training examples\")\n",
    "    axes.set_ylabel(\"Score\")\n",
    "\n",
    "    train_sizes, train_scores, test_scores  = \\\n",
    "        learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n",
    "                       train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    # Plot learning curve\n",
    "    axes.grid()\n",
    "    axes.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                         train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                         color=\"r\")\n",
    "    axes.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                         test_scores_mean + test_scores_std, alpha=0.1,\n",
    "                         color=\"g\")\n",
    "    axes.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "                 label=\"Training score\")\n",
    "    axes.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "                 label=\"Cross-validation score\")\n",
    "    axes.legend(loc=\"best\")\n",
    "    return plt\n",
    "\n",
    "\n",
    "# Colorplots features and boundaries\n",
    "\n",
    "def colorplot(clf, ax, x, y, h=100, precomputer=None):\n",
    "    '''\n",
    "    Overlay the decision areas as colors in an axes.\n",
    "    \n",
    "    Input:\n",
    "        clf: trained classifier\n",
    "        ax: axis to overlay color mesh on\n",
    "        x: feature on x-axis\n",
    "        y: feature on y-axis\n",
    "        h(optional): steps in the mesh\n",
    "    '''\n",
    "    # Create a meshgrid the size of the axis\n",
    "    xstep = (x.max() - x.min() ) / 20.0\n",
    "    ystep = (y.max() - y.min() ) / 20.0\n",
    "    x_min, x_max = x.min() - xstep, x.max() + xstep\n",
    "    y_min, y_max = y.min() - ystep, y.max() + ystep\n",
    "    h = max((x_max - x_min, y_max - y_min))/h\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    features = np.c_[xx.ravel(), yy.ravel()]\n",
    "    if precomputer is not None:\n",
    "        if type(precomputer) is RBFSampler:\n",
    "            features = precomputer.transform(features)\n",
    "        elif precomputer is rbf_kernel:\n",
    "            features = rbf_kernel(features, X)\n",
    "            \n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "    if hasattr(clf, \"decision_function\"):\n",
    "        Z = clf.decision_function(features)\n",
    "    else:\n",
    "        Z = clf.predict_proba(features)\n",
    "    if len(Z.shape) > 1:\n",
    "        Z = Z[:, 1]\n",
    "    \n",
    "    # Put the result into a color plot\n",
    "    cm = plt.cm.RdBu_r\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n",
    "    del xx, yy, x_min, x_max, y_min, y_max, Z, cm\n",
    "\n",
    "\n",
    "# Evaluation confusion matrix\n",
    "\n",
    "def evaluation(y_val, y_pred, title = 'Confusion Matrix'):\n",
    "    cm = confusion_matrix(y_val, y_pred)\n",
    "    precision = precision_score(y_val, y_pred)\n",
    "    recall = recall_score(y_val, y_pred)\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    f1 = f1_score(y_val, y_pred)\n",
    "    print('Recall: ', recall)\n",
    "    print('Accuracy: ', accuracy)\n",
    "    print('Precision: ', precision)\n",
    "    print('F1: ', f1)\n",
    "    sns.heatmap(cm,  cmap= 'PuBu', annot=True, fmt='g', annot_kws=    {'size':20})\n",
    "    plt.xlabel('predicted', fontsize=18)\n",
    "    plt.ylabel('actual', fontsize=18)\n",
    "    plt.title(title, fontsize=18)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Cross-validation pipeline\n",
    "\n",
    "def cross_validate(classifier, cv, x_train_trans, y_train_trans, x_val_trans, y_val_trans):\n",
    "\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocess', preprocessing_data),\n",
    "        ('classifier', classifier)\n",
    "    ])\n",
    "    train_acc = []\n",
    "    test_acc = []\n",
    "    for train_ind, val_ind in cv.split(X_train, y_train):\n",
    "        X_t, y_t = X_train.iloc[train_ind], y_train[train_ind]\n",
    "        pipeline.fit(X_t, y_t)\n",
    "        y_hat_t = pipeline.predict(X_t)\n",
    "        train_acc.append(accuracy_score(y_t, y_hat_t))\n",
    "        X_val, y_val = X_train.iloc[val_ind], y_train[val_ind]\n",
    "        y_hat_val = pipeline.predict(X_val)\n",
    "        test_acc.append(accuracy_score(y_val, y_hat_val))\n",
    "    print(evaluation(y_val, y_hat_val))\n",
    "    print('Training Accuracy: {}'.format(np.mean(train_acc)))\n",
    "    print('\\n')\n",
    "    print('Validation Accuracy: {}'.format(np.mean(test_acc)))\n",
    "    print('\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "from ecg.load_data import load_data\n",
    "data = load_data()\n",
    "\n",
    "x_train_trans, y_train, x_test_trans, y_test, x_val_trans, y_val = preprocessing_data(data)\n",
    "\n",
    "# X = x_train_trans\n",
    "# Y = y_train\n",
    "\n",
    "# Classifiers\n",
    "svmlin = SVC(kernel='linear', gamma='scale')\n",
    "svmrbf = SVC(kernel='rbf', gamma='scale')\n",
    "svmpoly = SVC(kernel='poly', degree=3, gamma='scale')\n",
    "clsfs = [KNeighborsClassifier(), RandomForestClassifier(), svmlin, svmpoly, svmrbf]\n",
    "classifier = svmlin\n"
   ]
  },
  {
   "source": [
    "# # Load data and create classifiers\n",
    "\n",
    "# # Data\n",
    "# from ecg.load_data import load_data\n",
    "# data = load_data()\n",
    "\n",
    "# x_train_trans, y_train, x_test_trans, y_test = preprocessing_data(data)\n",
    "\n",
    "# X = x_train_trans\n",
    "# Y = y_train\n",
    "\n",
    "# # Classifiers\n",
    "# # param_distributions_svm = {}\n",
    "# # param_distributions = {'n_estimators': randint(1, 400)}\n",
    "# # svmlin = RandomizedSearchCV(SVC(kernel='linear', gamma='scale'), param_distributions_svm, cv=5, n_iter=20, random_state=42)\n",
    "# # svmrbf = RandomizedSearchCV(SVC(kernel='rbf', gamma='scale'), param_distributions_svm, cv=5, n_iter=20, random_state=42)\n",
    "# # svmpoly = RandomizedSearchCV(SVC(kernel='poly', degree=3, gamma='scale'), param_distributions_svm, cv=5, n_iter=20, random_state=42)\n",
    "# # RF = RandomizedSearchCV(RandomForestClassifier(), param_distributions, cv=5, n_iter=20, random_state=42)\n",
    "# # LDA = RandomizedSearchCV(LinearDiscriminantAnalysis(), param_distributions, cv=5, n_iter=20, random_state=42)\n",
    "# # KNN = RandomizedSearchCV(KNeighborsClassifier(), param_distributions, cv=5, n_iter=20, random_state=42)\n",
    "# # clsfs = [svmlin,svmrbf,svmpoly,RF,LDA,KNN]\n",
    "# svmlin = SVC(kernel='linear', gamma='scale')\n",
    "# svmrbf = SVC(kernel='rbf', gamma='scale')\n",
    "# svmpoly = SVC(kernel='poly', degree=3, gamma='scale')\n",
    "# clsfs = [KNeighborsClassifier(), RandomForestClassifier(), svmlin, svmpoly, svmrbf]\n",
    "\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Learning Curve\n",
    "\n",
    "X = x_train_trans\n",
    "Y = y_train\n",
    "\n",
    "num = 0\n",
    "fig = plt.figure(figsize=(24,8*len(clsfs)))\n",
    "ax = fig.add_subplot(7, 3, num + 1)\n",
    "ax.scatter(X[:, 0], X[:, 1], marker='o', c=Y, \n",
    "           s=25, edgecolor='k', cmap=plt.cm.Paired)\n",
    "\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
    "num = 1\n",
    "for clf in clsfs:\n",
    "    # Split data in training and testing\n",
    "    title = str(clf)\n",
    "    ax = fig.add_subplot(7, 3, num + 1)\n",
    "    plot_learning_curve(clf, title, X, Y, ax, ylim=(0.3, 1.01), cv=cv)\n",
    "    num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-ba71c175f68c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m24\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclsfs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m ax.scatter(X[:, 0], X[:, 1], marker='o', c=Y, \n\u001b[0m\u001b[0;32m      7\u001b[0m            s=25, edgecolor='k', cmap=plt.cm.Paired)\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "# Plot classifications\n",
    "\n",
    "num = 0\n",
    "fig = plt.figure(figsize=(24,8*len(clsfs)))\n",
    "ax = fig.add_subplot(7, 3, num + 1)\n",
    "ax.scatter(X[:, 0], X[:, 1], marker='o', c=Y, \n",
    "           s=25, edgecolor='k', cmap=plt.cm.Paired)\n",
    "\n",
    "num = 1\n",
    "for clf in clsfs:\n",
    "    clf.fit(X, Y)\n",
    "    ax = fig.add_subplot(7, 3, num + 1)\n",
    "    ax.scatter(X[:, 0], X[:, 1], marker='o', c=Y,\n",
    "        s=25, edgecolor='k', cmap=plt.cm.Paired)\n",
    "    colorplot(clf, ax, X[:, 0], X[:, 1])\n",
    "    y_pred = clf.predict(x_val_trans)\n",
    "    accuracy = metrics.accuracy_score(y_val, y_pred)\n",
    "    best = clf.best_estimator_\n",
    "    t = ('Misclassified: %d / %d, %d' % ((y_val != y_pred).sum(), x_val_trans.shape[0], accuracy*100), str(clf))\n",
    "    ax.set_title(t)\n",
    "    print(best)\n",
    "    num += 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "get_params() missing 1 required positional argument: 'self'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-76a1124ff4b3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mLDA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: get_params() missing 1 required positional argument: 'self'"
     ]
    }
   ],
   "source": [
    "for param in LDA.get_params().keys():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}