{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Code opgeschoond en werkend_random.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "cF2F_jjxW_wK",
        "gFrCH_KzXOBE",
        "_AECMogeXS0K"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BrianvanWinden/TM10007_Group_13/blob/master/Code_opgeschoond_en_werkend_random.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFrCH_KzXOBE"
      },
      "source": [
        "# Importing functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5x3ZkH7XPa0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "827f35c1-cb7b-43e9-ac94-00064be96f93"
      },
      "source": [
        "# General packages\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn import datasets as ds\n",
        "from scipy.stats import randint\n",
        "from zipfile import ZipFile\n",
        "from scipy import stats\n",
        "from statistics import stdev\n",
        "\n",
        "\n",
        "# Classifiers and kernels\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn import preprocessing, metrics\n",
        "from sklearn.kernel_approximation import RBFSampler\n",
        "from sklearn.metrics.pairwise import rbf_kernel, sigmoid_kernel\n",
        "from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, accuracy_score, f1_score, roc_auc_score\n",
        "\n",
        "# Regularization\n",
        "from sklearn.linear_model import Lasso, RidgeClassifier\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "# Model selection\n",
        "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, train_test_split, learning_curve, ShuffleSplit, StratifiedKFold\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, roc_curve, auc\n",
        "\n",
        "# Sampling\n",
        "from imblearn.combine import SMOTEENN\n",
        "from imblearn.under_sampling import EditedNearestNeighbours, RandomUnderSampler\n",
        "\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "\n",
        "\n",
        "def evaluation(y_val, y_pred, title = 'Confusion Matrix'):\n",
        "    cm = confusion_matrix(y_val, y_pred)\n",
        "    precision = precision_score(y_val, y_pred)\n",
        "    recall = recall_score(y_val, y_pred)\n",
        "    accuracy = accuracy_score(y_val, y_pred)\n",
        "    f1 = f1_score(y_val, y_pred)\n",
        "    # print('Recall: ', recall)\n",
        "    # print('Accuracy: ', accuracy)\n",
        "    # print('Precision: ', precision)\n",
        "    # print('F1: ', f1)\n",
        "    sns.heatmap(cm,  cmap= 'PuBu', annot=True, fmt='g', annot_kws=    {'size':20})\n",
        "    plt.xlabel('predicted', fontsize=18)\n",
        "    plt.ylabel('actual', fontsize=18)\n",
        "    plt.title(title, fontsize=18)\n",
        "\n",
        "def plot_learning_curve(estimator, title, X, y, axes, ylim=None, cv=None,\n",
        "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
        "    \n",
        "\n",
        "    axes.set_title(title)\n",
        "    if ylim is not None:\n",
        "        axes.set_ylim(*ylim)\n",
        "    axes.set_xlabel(\"Training examples\")\n",
        "    axes.set_ylabel(\"Score\")\n",
        "\n",
        "    train_sizes, train_scores, test_scores  = \\\n",
        "        learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n",
        "                       train_sizes=train_sizes)\n",
        "    train_scores_mean = np.mean(train_scores, axis=1)\n",
        "    train_scores_std = np.std(train_scores, axis=1)\n",
        "    test_scores_mean = np.mean(test_scores, axis=1)\n",
        "    test_scores_std = np.std(test_scores, axis=1)\n",
        "\n",
        "    # Plot learning curve\n",
        "    axes.grid()\n",
        "    axes.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
        "                         train_scores_mean + train_scores_std, alpha=0.1,\n",
        "                         color=\"r\")\n",
        "    axes.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
        "                         test_scores_mean + test_scores_std, alpha=0.1,\n",
        "                         color=\"g\")\n",
        "    axes.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
        "                 label=\"Training score\")\n",
        "    axes.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
        "                 label=\"Cross-validation score\")\n",
        "    axes.legend(loc=\"best\")\n",
        "    return plt\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def roc_plotter(fpr, tpr, roc_auc):\n",
        "    \"\"\" Plot the ROC curve of the classifier for the given number of components\n",
        "\n",
        "    - param numpy.array fpr: False positive rate\n",
        "    - param numpy.array tpr: True positive rate\n",
        "    - param integer roc_auc: Area under the ROC curve\n",
        "    \"\"\"\n",
        "    plt.plot(fpr, tpr, linewidth=2, label=f'ROC curve (area = {roc_auc:0.2f})')\n",
        "\n",
        "    # Properties ROC curve figure\n",
        "    plt.plot([0, 1], [0, 1], color='navy', linewidth=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('ROC curve')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cF2F_jjxW_wK"
      },
      "source": [
        "# Assignment arrhythmia classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1-5TqoFW8_e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2700bd5d-b299-4060-afa5-a7a7887a6402"
      },
      "source": [
        "# Run this to use from colab environment\n",
        "!git clone https://github.com/BrianvanWinden/TM10007_Group_13.git\n",
        "!unzip \"/content/TM10007_Group_13/ecg/ecg_data.zip\"\n",
        "\n",
        "data = pd.read_csv(\"ecg_data.csv\") \n",
        "\n",
        "data_points_org= data.drop(['label', 'Unnamed: 0'], axis=1).to_numpy()\n",
        "data_labels_org= data['label'].to_numpy()\n",
        "\n",
        "# data_points = np.delete(data_points_org, [91, 284, 422, 611] , axis=0)\n",
        "# data_labels = np.delete(data_labels_org, [91, 284, 422, 611] , axis=0)\n",
        "\n",
        "data_points = data_points_org\n",
        "data_labels = data_labels_org\n",
        "\n",
        "print (data_labels.shape)\n",
        "print (data_points.shape)\n",
        "\n",
        "\n",
        "x_train_val, x_test, y_train_val, y_test = train_test_split(data_points, data_labels, test_size=0.2, stratify=data_labels)\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'TM10007_Group_13'...\n",
            "remote: Enumerating objects: 123, done.\u001b[K\n",
            "remote: Counting objects: 100% (123/123), done.\u001b[K\n",
            "remote: Compressing objects: 100% (107/107), done.\u001b[K\n",
            "remote: Total 123 (delta 57), reused 47 (delta 14), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (123/123), 69.33 MiB | 35.39 MiB/s, done.\n",
            "Resolving deltas: 100% (57/57), done.\n",
            "Archive:  /content/TM10007_Group_13/ecg/ecg_data.zip\n",
            "  inflating: ecg_data.csv            \n",
            "(827,)\n",
            "(827, 9000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AECMogeXS0K"
      },
      "source": [
        "# Taking a look at the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "WQkqAm3RXWhd",
        "outputId": "a7345157-46fa-4500-af3b-cf1142fb6e75"
      },
      "source": [
        "# Samples and features\n",
        "# print(data)\n",
        "print(f'The number of samples: {len(data.index)}')\n",
        "shape_data = data_points.shape\n",
        "print(f'The number of features: {shape_data[1]}')\n",
        "total_datapoints = shape_data[0]*shape_data[1]\n",
        "print(f'Total number of datapoints: {total_datapoints}')\n",
        "healthy_patients = (list(data['label'] == 0)).count(True)\n",
        "print(f'The number of healthy patients: {healthy_patients}') \n",
        "sick_patients = (list(data['label'] == 1)).count(True)\n",
        "print(f'The number of sick patients: {sick_patients}')\n",
        "percentage_sick=sick_patients/(sick_patients+healthy_patients)\n",
        "print(f'Percentage of sick patients: {round(percentage_sick,2)*100}%')\n",
        "\n",
        "# Determine Z-scores \n",
        "z = np.abs(stats.zscore(data_points_org))\n",
        "outliers = np.where(z>3)\n",
        "number_outliers = len(outliers[1])\n",
        "print(f'Number of datapoints where the Z score is larger than 3: {number_outliers}')\n",
        "\n",
        "\n",
        "percentage = number_outliers/total_datapoints*100\n",
        "print(f'Percentage of outliers in total data: {round(percentage,2)}%')\n",
        "\n",
        "# Missing values\n",
        "missing_values = data.isna().sum()\n",
        "number_missing_values = missing_values.astype(bool).sum(axis=0)\n",
        "print(f'Number of missing data points:{number_missing_values}')\n",
        "\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of samples: 827\n",
            "The number of features: 9000\n",
            "Total number of datapoints: 7443000\n",
            "The number of healthy patients: 681\n",
            "The number of sick patients: 146\n",
            "Percentage of sick patients: 18.0%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-e025656a8035>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_points_org\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0moutliers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mnumber_outliers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Number of datapoints where the Z score is larger than 3: {number_outliers}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vy0-beOPbAgL"
      },
      "source": [
        "# Learning curves"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ex7Fn9pubAGy"
      },
      "source": [
        "# Learning Curve\n",
        "\n",
        "\n",
        "clsfs = {'linSVC': LinearSVC(class_weight='balanced', dual=False, max_iter=10000), \n",
        "          'SVMrbf':SVC(kernel='rbf', class_weight='balanced'), \n",
        "          'SVMlin': SVC(kernel='linear', class_weight='balanced'), \n",
        "          'Logreg': LogisticRegression(class_weight='balanced', dual=False, max_iter=10000),\n",
        "          'knn': KNeighborsClassifier(weights='distance')}\n",
        "                  \n",
        "\n",
        "num = 0\n",
        "fig = plt.figure(figsize=(24,8*len(clsfs)))\n",
        "ax = fig.add_subplot(7, 3, num + 1)\n",
        "ax.scatter(data_points[:, 0], data_points[:, 1], marker='o', c=data_labels, \n",
        "           s=25, edgecolor='k', cmap=plt.cm.Paired)\n",
        "\n",
        "cv = ShuffleSplit(n_splits=2, test_size=0.2, random_state=0)\n",
        "num = 1\n",
        "for key, clf in clsfs.items():\n",
        "    title = str(key)\n",
        "    ax = fig.add_subplot(7, 3, num + 1)\n",
        "    plot_learning_curve(clf, title, data_points, data_labels, ax, ylim=(0.3, 1.01), cv=cv)\n",
        "    num += 1\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMk1Tg7z_dFe"
      },
      "source": [
        "# PCA plot\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkJ8qLa1_iti"
      },
      "source": [
        "pca = PCA(n_components=0.99)\n",
        "pca = pca.fit(data_points)\n",
        "x_trans = pca.transform(data_points)\n",
        "y = data_labels\n",
        "\n",
        "plt.figure(0)\n",
        "sns.scatterplot(x=x_trans[:, 0], y=x_trans[:, 1], hue=y) # Scatterplot of two first PCs\n",
        "plt.xlabel('PC1')\n",
        "plt.ylabel('PC2')\n",
        "plt.title('Scatterplot of two first PCs')\n",
        "\n",
        "plt.figure(1)\n",
        "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "ratio = pca.explained_variance_ratio_.cumsum()\n",
        "plt.xlabel('Number of components')\n",
        "plt.ylabel('Cumulative explained variance')\n",
        "plt.title('Explained variance for number of principle components')\n",
        "plt.show\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0ddx2i7Xajb"
      },
      "source": [
        "# Preprocessing and classifier validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaqV1Y1YXbar"
      },
      "source": [
        "# Splitting data  \n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
        "\n",
        "cv_outer = StratifiedKFold(n_splits=5, shuffle=True)\n",
        "cv_inner = StratifiedKFold(n_splits=3, shuffle=True)\n",
        "outer_results = list()\n",
        "all_scores = list()\n",
        "models = list()\n",
        "\n",
        "for train_ix, val_ix in cv_outer.split(x_train_val,y_train_val):\n",
        "  # split data\n",
        "  x_train, x_val = x_train_val[train_ix, :], x_train_val[val_ix, :]\n",
        "  y_train, y_val = y_train_val[train_ix], y_train_val[val_ix]\n",
        "  print (x_train.shape)\n",
        "  print (x_val.shape)\n",
        "  #Resampling\n",
        "  resample = RandomUnderSampler(sampling_strategy='majority')\n",
        "  x_resampled, y_resampled = resample.fit_resample(x_train, y_train)\n",
        "\n",
        "\n",
        "  #Scaling\n",
        "  scaler = preprocessing.RobustScaler()\n",
        "  scaler.fit(x_resampled)\n",
        "  x_train_scaled = scaler.transform(x_resampled)\n",
        "  x_val_scaled = scaler.transform(x_val)\n",
        "\n",
        "  #PCA\n",
        "  pca = PCA(n_components=0.99)\n",
        "  pca = pca.fit(x_train_scaled)\n",
        "  x_train_trans = pca.transform(x_train_scaled)\n",
        "  x_val_trans = pca.transform(x_val_scaled)\n",
        "  print(f'Amount of trainingsamples after resampling: {x_train_trans.shape[0]}')\n",
        "  print(f'Amount of features from PCA: {x_train_trans.shape[1]}')\n",
        "  model_params = {\n",
        "                  'svm_rbf': \n",
        "                  {'model': SVC(kernel='rbf', class_weight='balanced'),\n",
        "                    'params': {'C': np.arange(0.01,100,0.01),\n",
        "                               'gamma': np.arange(0.01,100,0.01)\n",
        "                               }\n",
        "                  },\n",
        "                   'svm_lin': \n",
        "                  {'model': SVC(kernel='linear', class_weight='balanced'),\n",
        "                    'params': {'C': np.arange(0.01,100,0.01)\n",
        "                               }\n",
        "                   },\n",
        "                  'linsvc': \n",
        "                  {'model': LinearSVC(class_weight='balanced', dual=False, max_iter=10000),\n",
        "                   'params':{'C': np.arange(0.01,100,0.01)\n",
        "                             }\n",
        "                  },\n",
        "                  'logreg':\n",
        "                  {'model': LogisticRegression(class_weight='balanced', dual=False, max_iter=10000),\n",
        "                   'params':{'C': np.arange(0.01,100,0.01)\n",
        "                             }                      \n",
        "                  },\n",
        "                  'knn':\n",
        "                  {'model': KNeighborsClassifier(weights='distance'),\n",
        "                   'params':{\n",
        "                        'n_neighbors': [5,10,15],\n",
        "                        'leaf_size': np.arange(1,100,1)\n",
        "                             }\n",
        "                   }\n",
        "                }\n",
        "\n",
        "  for model_name, mp in model_params.items():\n",
        "    search = RandomizedSearchCV(mp['model'], mp['params'], cv = cv_inner, scoring = 'roc_auc', return_train_score=True)\n",
        "    result = search.fit(x_train_scaled, y_resampled)\n",
        "    best_model = result.best_estimator_\n",
        "    models.append(best_model)\n",
        "    yhat = best_model.predict(x_val_scaled)\n",
        "    auc_score_val = roc_auc_score(y_val, yhat)\n",
        "    outer_results.append(auc_score_val)\n",
        "    all_scores.append(result.best_score_)\n",
        "    (f'Model:{model_name}, best parameters:{result.best_params_}, auc: {round(auc_score_val, 3)}, result.best_score_:{round(result.best_score_, 3)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C10micdFcCng"
      },
      "source": [
        "# Best classifier for test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqnxvM-GcE4T"
      },
      "source": [
        "from imblearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "models_df = pd.DataFrame(models)\n",
        "outer_results_df = pd.DataFrame(outer_results)\n",
        "all_scores_df = pd.DataFrame(all_scores)\n",
        "results = pd.concat([outer_results_df, all_scores_df, models_df], axis=1)\n",
        "results.columns = [ 'Auc score', 'Best score', 'Model']\n",
        "results_sorted = results.sort_values(by=['Auc score'], ascending=False)\n",
        "display(results_sorted)\n",
        "\n",
        "best_classifier = results_sorted['Model'].iloc[0]\n",
        "print(f'The best classifier is: {best_classifier}')\n",
        "\n",
        "\n",
        "#Resampling\n",
        "resample = RandomUnderSampler(sampling_strategy='majority')\n",
        "X_resampled, y_resampled = resample.fit_resample(x_train_val, y_train_val)\n",
        "\n",
        "#Scaling\n",
        "scaler = preprocessing.RobustScaler()\n",
        "scaler.fit(X_resampled)\n",
        "x_train_scaled = scaler.transform(X_resampled)\n",
        "x_test_scaled = scaler.transform(x_test)\n",
        "\n",
        "#PCA\n",
        "pca = PCA(n_components=0.99)\n",
        "pca = pca.fit(x_train_scaled)\n",
        "x_train_trans = pca.transform(x_train_scaled)\n",
        "x_test_trans = pca.transform(x_test_scaled)\n",
        "\n",
        "# Evaluation classifier\n",
        "clf = best_classifier.fit(x_train_trans, y_resampled)\n",
        "y_pred = clf.predict(x_test_trans)\n",
        "y_score = clf.decision_function(x_test_trans)\n",
        "accuracy = metrics.balanced_accuracy_score(y_test, y_pred)\n",
        "auc_score = metrics.roc_auc_score(y_test, y_pred)\n",
        "print('Misclassified: %d / %d' % ((y_test != y_pred).sum(), x_test_trans.shape[0]))\n",
        "evaluation(y_test, y_pred)\n",
        "\n",
        "print(f'Auc score test data: {auc_score}')\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9nkWQVZZJgg"
      },
      "source": [
        "# ROC curve"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVTzsMfOZLBw"
      },
      "source": [
        "fpr = dict()\n",
        "tpr = dict()\n",
        "roc_auc = dict()\n",
        "fpr, tpr, _ = roc_curve(y_test, y_score)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "roc_plotter(fpr, tpr, roc_auc)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}