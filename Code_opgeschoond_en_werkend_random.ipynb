{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Code opgeschoond en werkend_random.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "cF2F_jjxW_wK",
        "gFrCH_KzXOBE",
        "_AECMogeXS0K"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BrianvanWinden/TM10007_Group_13/blob/master/Code_opgeschoond_en_werkend_random.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFrCH_KzXOBE"
      },
      "source": [
        "# Importing functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5x3ZkH7XPa0"
      },
      "source": [
        "# General packages\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn import datasets as ds\n",
        "from scipy.stats import randint\n",
        "from zipfile import ZipFile\n",
        "from scipy import stats\n",
        "from statistics import stdev\n",
        "\n",
        "\n",
        "# Classifiers and kernels\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn import preprocessing, metrics\n",
        "from sklearn.kernel_approximation import RBFSampler\n",
        "from sklearn.metrics.pairwise import rbf_kernel, sigmoid_kernel\n",
        "from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, accuracy_score, f1_score, roc_auc_score\n",
        "\n",
        "# Regularization\n",
        "from sklearn.linear_model import Lasso, RidgeClassifier\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "# Model selection\n",
        "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, train_test_split, learning_curve, ShuffleSplit, StratifiedKFold\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, roc_curve, auc\n",
        "\n",
        "# Sampling\n",
        "from imblearn.combine import SMOTEENN\n",
        "from imblearn.under_sampling import EditedNearestNeighbours\n",
        "\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "\n",
        "\n",
        "def evaluation(y_val, y_pred, title = 'Confusion Matrix'):\n",
        "    cm = confusion_matrix(y_val, y_pred)\n",
        "    precision = precision_score(y_val, y_pred)\n",
        "    recall = recall_score(y_val, y_pred)\n",
        "    accuracy = accuracy_score(y_val, y_pred)\n",
        "    f1 = f1_score(y_val, y_pred)\n",
        "    # print('Recall: ', recall)\n",
        "    # print('Accuracy: ', accuracy)\n",
        "    # print('Precision: ', precision)\n",
        "    # print('F1: ', f1)\n",
        "    sns.heatmap(cm,  cmap= 'PuBu', annot=True, fmt='g', annot_kws=    {'size':20})\n",
        "    plt.xlabel('predicted', fontsize=18)\n",
        "    plt.ylabel('actual', fontsize=18)\n",
        "    plt.title(title, fontsize=18)\n",
        "\n",
        "def plot_learning_curve(estimator, title, X, y, axes, ylim=None, cv=None,\n",
        "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
        "    \n",
        "\n",
        "    axes.set_title(title)\n",
        "    if ylim is not None:\n",
        "        axes.set_ylim(*ylim)\n",
        "    axes.set_xlabel(\"Training examples\")\n",
        "    axes.set_ylabel(\"Score\")\n",
        "\n",
        "    train_sizes, train_scores, test_scores  = \\\n",
        "        learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n",
        "                       train_sizes=train_sizes)\n",
        "    train_scores_mean = np.mean(train_scores, axis=1)\n",
        "    train_scores_std = np.std(train_scores, axis=1)\n",
        "    test_scores_mean = np.mean(test_scores, axis=1)\n",
        "    test_scores_std = np.std(test_scores, axis=1)\n",
        "\n",
        "    # Plot learning curve\n",
        "    axes.grid()\n",
        "    axes.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
        "                         train_scores_mean + train_scores_std, alpha=0.1,\n",
        "                         color=\"r\")\n",
        "    axes.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
        "                         test_scores_mean + test_scores_std, alpha=0.1,\n",
        "                         color=\"g\")\n",
        "    axes.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
        "                 label=\"Training score\")\n",
        "    axes.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
        "                 label=\"Cross-validation score\")\n",
        "    axes.legend(loc=\"best\")\n",
        "    return plt\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def roc_plotter(fpr, tpr, roc_auc):\n",
        "    \"\"\" Plot the ROC curve of the classifier for the given number of components\n",
        "\n",
        "    - param numpy.array fpr: False positive rate\n",
        "    - param numpy.array tpr: True positive rate\n",
        "    - param integer roc_auc: Area under the ROC curve\n",
        "    \"\"\"\n",
        "    plt.plot(fpr, tpr, linewidth=2, label=f'ROC curve (area = {roc_auc:0.2f})')\n",
        "\n",
        "    # Properties ROC curve figure\n",
        "    plt.plot([0, 1], [0, 1], color='navy', linewidth=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('ROC curve')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cF2F_jjxW_wK"
      },
      "source": [
        "# Assignment arrhythmia classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1-5TqoFW8_e"
      },
      "source": [
        "# Run this to use from colab environment\n",
        "!git clone https://github.com/BrianvanWinden/TM10007_Group_13.git\n",
        "!unzip \"/content/TM10007_Group_13/ecg/ecg_data.zip\"\n",
        "import pandas as pd \n",
        "\n",
        "data = pd.read_csv(\"ecg_data.csv\") \n",
        "\n",
        "data_points= data.drop(['label', 'Unnamed: 0'], axis=1).to_numpy()\n",
        "data_labels= data['label'].to_numpy()\n",
        "\n",
        "\n",
        "x_train_val, x_test, y_train_val, y_test = train_test_split(data_points, data_labels, test_size=0.3, stratify=data_labels)\n",
        "\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AECMogeXS0K"
      },
      "source": [
        "# Taking a look at the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQkqAm3RXWhd",
        "outputId": "766546e6-c352-4e98-8063-8ec050403259"
      },
      "source": [
        "# Samples and features\n",
        "print(data.head)\n",
        "print(f'The number of samples: {len(data.index)}')\n",
        "print(f'The number of features: {len(data.columns)}')\n",
        "healthy_patients = (list(data['label'] == 0)).count(True)\n",
        "print(f'The number of healthy patients: {healthy_patients}') \n",
        "sick_patients = (list(data['label'] == 1)).count(True)\n",
        "print(f'The number of sick patients: {sick_patients}')\n",
        "percentage_sick=sick_patients/(sick_patients+healthy_patients)\n",
        "print(f'Percentage of sick patients: {round(percentage_sick,2)*100}%')\n",
        "\n",
        "# Outliers # aanpassen\n",
        "z = np.abs(stats.zscore(data))\n",
        "x = np.where(z>3)\n",
        "print(f'Number of datapoints where the Z score is larger than 3: {len(x[1])}')\n",
        "\n",
        "# Missing values\n",
        "missing_values = data.isna().sum()\n",
        "number_missing_values = missing_values.astype(bool).sum(axis=0)\n",
        "print(f'Number of missig data points:{number_missing_values}')\n",
        "\n",
        "\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<bound method NDFrame.head of      Unnamed: 0         0_0         0_1  ...    11_748    11_749  label\n",
            "0             0   47.685046   48.416904  ...  0.269249  0.112248      0\n",
            "1             1  152.726718  297.353726  ...  0.859936  0.742673      1\n",
            "2             2    1.601260    3.882169  ...  0.098506  0.098288      0\n",
            "3             3    1.388947    3.052483  ...  0.092000  0.192559      0\n",
            "4             4    3.625561    3.728466  ...  0.141610  0.129477      0\n",
            "..          ...         ...         ...  ...       ...       ...    ...\n",
            "822         822   14.673713   13.879148  ...  0.155580  0.061931      0\n",
            "823         823    3.167367   13.771749  ...  0.771741  0.757218      0\n",
            "824         824    0.439357    8.700374  ...  0.159615  0.077114      0\n",
            "825         825    0.235515    4.074581  ...  0.060548  0.116350      0\n",
            "826         826    0.301328    2.012000  ...  0.114943  0.223285      1\n",
            "\n",
            "[827 rows x 9002 columns]>\n",
            "The number of samples: 827\n",
            "The number of features: 9002\n",
            "The number of healthy patients: 681\n",
            "The number of sick patients: 146\n",
            "Percentage of sick patients: 18.0%\n",
            "Number of datapoints where the Z score is larger than 3: 57812\n",
            "Number of missig data points:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWhxoFCqm8eq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0ddx2i7Xajb"
      },
      "source": [
        "# Preprocessing and classifier validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CCMjQG434ZV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zaqV1Y1YXbar",
        "outputId": "45243201-47dd-4851-abbe-5f55b5c32898"
      },
      "source": [
        "# Splitting data  \n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
        "\n",
        "cv_outer = StratifiedKFold(n_splits=5, shuffle=True)\n",
        "cv_inner = StratifiedKFold(n_splits=3, shuffle=True)\n",
        "outer_results = list()\n",
        "all_scores = list()\n",
        "models = list()\n",
        "\n",
        "for train_ix, val_ix in cv_outer.split(x_train_val,y_train_val):\n",
        "  # split data\n",
        "  X_train, X_val = x_train_val[train_ix, :], x_train_val[val_ix, :]\n",
        "  y_train, y_val = y_train_val[train_ix], y_train_val[val_ix]\n",
        "  print (X_train.shape)\n",
        "  print(X_val.shape)\n",
        "  #Resampling\n",
        "  # combines SMOTE over and edited nearest neighbour undersampling\n",
        "  # edited nearest neighbours kan van beide groepen weghalen, met sampling_strategy='majority' alleen van grote groep\n",
        "  # the authors comment that ENN is more aggressive at downsampling the majority class than Tomek Links, providing more in-depth cleaning. They apply the method, removing examples from both the majority and minority classes.\n",
        "  #A Study of the Behavior of Several Methods for Balancing Machine Learning Training Data, 2004.\n",
        "  # resample = SMOTEENN(enn=EditedNearestNeighbours(sampling_strategy='majority'))\n",
        "  # X_resampled, y_resampled = resample.fit_resample(X_train, y_train)\n",
        "  X_resampled, y_resampled = X_train, y_train\n",
        "\n",
        "  #Scaling\n",
        "  scaler = preprocessing.RobustScaler()\n",
        "  scaler.fit(X_resampled)\n",
        "  x_train_scaled = scaler.transform(X_resampled)\n",
        "  x_val_scaled = scaler.transform(X_val)\n",
        "\n",
        "  #PCA\n",
        "  pca = PCA(n_components=0.99)\n",
        "  pca = pca.fit(x_train_scaled)\n",
        "  x_train_trans = pca.transform(x_train_scaled)\n",
        "  x_val_trans = pca.transform(x_val_scaled)\n",
        "  print(f'Amount of features from PCA: {x_train_trans.shape[1]}')\n",
        "  model_params = {\n",
        "                  'svm_rbf': \n",
        "                  {'model': SVC(kernel='rbf', class_weight='balanced'),\n",
        "                    'params': {'C': np.arange(0.01,100,0.01),\n",
        "                               'gamma': np.arange(0.01,100,0.01)\n",
        "                               }\n",
        "                  },\n",
        "                   'svm_lin': \n",
        "                  {'model': SVC(kernel='linear', class_weight='balanced'),\n",
        "                    'params': {'C': np.arange(0.01,100,0.01)\n",
        "                               }\n",
        "                   },\n",
        "                  'linsvc': \n",
        "                  {'model': LinearSVC(class_weight='balanced', dual=False, max_iter=10000),\n",
        "                   'params':{'C': np.arange(0.01,100,0.01)\n",
        "                             }\n",
        "                  },\n",
        "                  'logreg':\n",
        "                  {'model': LogisticRegression(class_weight='balanced', dual=False, max_iter=10000),\n",
        "                   'params':{'C': np.arange(0.01,100,0.01)\n",
        "                             }                      \n",
        "                  # # },\n",
        "                  # # 'lda': \n",
        "                  # # {'model': LinearDiscriminantAnalysis(),\n",
        "                  # # 'params':{}\n",
        "                  # # },\n",
        "                  # # 'knn':\n",
        "                  # # {'model': KNeighborsClassifier(weights='distance'),\n",
        "                  # #  'params':{'n_neighbors': [5,10,15]\n",
        "                  # #            }\n",
        "                   }\n",
        "                }\n",
        "\n",
        "  for model_name, mp in model_params.items():\n",
        "    search = RandomizedSearchCV(mp['model'], mp['params'], cv = cv_inner, scoring = 'roc_auc', return_train_score=True)\n",
        "    result = search.fit(x_train_scaled, y_resampled)\n",
        "    best_model = result.best_estimator_\n",
        "    models.append(best_model)\n",
        "    yhat = best_model.predict(x_val_scaled)\n",
        "    auc_score_val = roc_auc_score(y_val, yhat)\n",
        "    outer_results.append(auc_score_val)\n",
        "    all_scores.append(result.best_score_)\n",
        "    print(f'auc: {auc_score_val}, result.best_score_:{result.best_score_}, best parameters:{result.best_params_}')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(462, 9001)\n",
            "(116, 9001)\n",
            "Amount of features from PCA: 278\n",
            "auc: 0.5, result.best_score_:0.5, best parameters:{'gamma': 60.72, 'C': 36.62}\n",
            "auc: 0.7541666666666668, result.best_score_:0.8150315535954831, best parameters:{'C': 71.7}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C10micdFcCng"
      },
      "source": [
        "# Best classifier for test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqnxvM-GcE4T"
      },
      "source": [
        "from imblearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "models_df = pd.DataFrame(models)\n",
        "outer_results_df = pd.DataFrame(outer_results)\n",
        "all_scores_df = pd.DataFrame(all_scores)\n",
        "results = pd.concat([models_df, outer_results_df, all_scores_df], axis=1)\n",
        "results.columns = ['Models', 'Auc score', 'Best score']\n",
        "results_sorted = results.sort_values(by=['Auc score'], ascending=False)\n",
        "print(results_sorted)\n",
        "\n",
        "best_classifier = results_sorted['Models'].iloc[0]\n",
        "print(f'The best classifier is: {best_classifier}')\n",
        "\n",
        "# cv_test = StratifiedKFold(n_splits=2, shuffle=True)\n",
        "# # ('resampling', SMOTEENN(enn=EditedNearestNeighbours(sampling_strategy='majority'))),('scaling', preprocessing.RobustScaler()),('pca', PCA(n_components=0.99)),\n",
        "# clf = Pipeline([('pca', PCA(n_components=0.99)), ('clf', best_classifier)])\n",
        "# cross_score = cross_val_score(clf, x_test, y_test, cv=cv_test, scoring='roc_auc')\n",
        "# print(f'Mean score of classifier on test data: {round(mean(cross_score),3)} + {round(stdev(cross_score),3)}')\n",
        "\n",
        "#Resampling\n",
        "resample = SMOTEENN(enn=EditedNearestNeighbours(sampling_strategy='majority'))\n",
        "X_resampled, y_resampled = resample.fit_resample(x_train_val, y_train_val)\n",
        "\n",
        "#Scaling\n",
        "scaler = preprocessing.RobustScaler()\n",
        "scaler.fit(X_resampled)\n",
        "x_train_scaled = scaler.transform(X_resampled)\n",
        "x_test_scaled = scaler.transform(x_test)\n",
        "\n",
        "\n",
        "#PCA\n",
        "pca = PCA(n_components=0.99)\n",
        "pca = pca.fit(x_train_scaled)\n",
        "x_train_trans = pca.transform(x_train_scaled)\n",
        "x_test_trans = pca.transform(x_test_scaled)\n",
        "\n",
        "# Evaluation classifier\n",
        "clf = best_classifier.fit(x_train_trans, y_resampled)\n",
        "y_pred = clf.predict(x_test_trans)\n",
        "y_score = clf.decision_function(x_test_trans)\n",
        "accuracy = metrics.balanced_accuracy_score(y_test, y_pred)\n",
        "auc_score = metrics.roc_auc_score(y_test, y_pred)\n",
        "print('Misclassified: %d / %d' % ((y_test != y_pred).sum(), x_test_trans.shape[0]), str(clf))\n",
        "evaluation(y_test, y_pred)\n",
        "\n",
        "print(f'Auc score test data: {auc_score}')\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9nkWQVZZJgg"
      },
      "source": [
        "# ROC curve"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVTzsMfOZLBw"
      },
      "source": [
        "fpr = dict()\n",
        "tpr = dict()\n",
        "roc_auc = dict()\n",
        "fpr, tpr, _ = roc_curve(y_test, y_score)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "roc_plotter(fpr, tpr, roc_auc)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vy0-beOPbAgL"
      },
      "source": [
        "# Learning curves"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ex7Fn9pubAGy"
      },
      "source": [
        "# Learning Curve\n",
        "\n",
        "X = data_points\n",
        "Y = data_labels\n",
        "\n",
        "clsfs = {'linSVC': LinearSVC(class_weight='balanced', dual=False, max_iter=10000), \n",
        "          'SVMrbf':SVC(kernel='rbf', class_weight='balanced'), \n",
        "          'SVMlin': SVC(kernel='linear', class_weight='balanced'), \n",
        "          'Logreg': LogisticRegression(class_weight='balanced', dual=False, max_iter=10000)}\n",
        "\n",
        "num = 0\n",
        "fig = plt.figure(figsize=(24,8*len(clsfs)))\n",
        "ax = fig.add_subplot(7, 3, num + 1)\n",
        "ax.scatter(X[:, 0], X[:, 1], marker='o', c=Y, \n",
        "           s=25, edgecolor='k', cmap=plt.cm.Paired)\n",
        "\n",
        "cv = ShuffleSplit(n_splits=2, test_size=0.2, random_state=0)\n",
        "num = 1\n",
        "for key, clf in clsfs.items():\n",
        "    title = str(key)\n",
        "    ax = fig.add_subplot(7, 3, num + 1)\n",
        "    plot_learning_curve(clf, title, X, Y, ax, ylim=(0.3, 1.01), cv=cv)\n",
        "    num += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNr-U4_DQP4u"
      },
      "source": [
        "# ROC curve\n"
      ]
    }
  ]
}